{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Multilabel Classification for Toxic Comments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dei dati\n",
    "Il dataset di partenza √® stato preso da una [challenge](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/) pubblicata su [Kaggle](https://www.kaggle.com/) nel 2018 da [Jigsaw](https://jigsaw.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data + labels\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "test_wlabels = test_data.merge(test_labels)\n",
    "for s in labels:\n",
    "    test_wlabels = test_wlabels[test_wlabels[s] != -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis (pt.1) e subsampling\n",
    "Il dataset risulta essere molto sbilanciato, per questo motivo √® stato optato per un subsampling della classe dominante, che risulta essere quella con tutte le classi poste a zero.  \n",
    "Si √® scelto di utilizzare la proporzione 50/50, dove met√† del dataset risultano essere commenti \"puliti\", mentre l'altra met√† dei record ha un qualche grado di tossicit√†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_ppl_counter(data):\n",
    "    count = 0\n",
    "    for d in range(len(data)):\n",
    "        if data['toxic'][d] == 0 and data['severe_toxic'][d] == 0 and data['obscene'][d] == 0 and data['threat'][d] == 0 and data['insult'][d] == 0 and data['identity_hate'][d] == 0:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def bad_ppl_counter(data):\n",
    "    count = 0\n",
    "    for d in range(len(data)):\n",
    "        for s in labels:\n",
    "            if data[s][d] == 1:\n",
    "                count += 1\n",
    "                break\n",
    "    return count\n",
    "\n",
    "# Length of the train data\n",
    "print(f'Lenght of train data: {len(train_data)}')\n",
    "print(f'Good people: {good_ppl_counter(train_data)}')\n",
    "print(f'Bad people: {bad_ppl_counter(train_data)}')\n",
    "print('..........')\n",
    "\n",
    "# Record count for each label\n",
    "for s in labels:\n",
    "    print(f'{s}: {len(train_data[train_data[s] == 1])}')\n",
    "print('..........')\n",
    "\n",
    "# Record count for each label with toxic = 0\n",
    "count = {}\n",
    "for s in labels:\n",
    "    count[s] = 0\n",
    "for d in range(len(train_data)):\n",
    "    for s in labels:\n",
    "        if train_data['toxic'][d] == 0 and train_data[s][d] == 1:\n",
    "            count[s] += 1\n",
    "for k in count.keys():\n",
    "    print(f'{k}: {count[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(train_data, test_size=0.05)\n",
    "train_toxic = train_df[train_df[labels].sum(axis=1) > 0]\n",
    "train_clean = train_df[train_df[labels].sum(axis=1) == 0]\n",
    "\n",
    "train_df = pd.concat([\n",
    "  train_toxic,\n",
    "  train_clean.sample(20000)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete from dataframe the rows with nan values\n",
    "test_data = test_data[~test_data['text_lem'].isna()]\n",
    "test_data['text_lem'].isna().sum()\n",
    "train_data = train_data[~train_data['text_lem'].isna()]\n",
    "train_data['text_lem'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the train data\n",
    "print(f'Lenght of train data: {len(train_data)}')\n",
    "print(f'Good people: {good_ppl_counter(train_data)}')\n",
    "print(f'Bad people: {bad_ppl_counter(train_data)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Per poter operare dei dati bisogna prima effettuare delle operazioni di preprocessing.  \n",
    "Il nostro preprocessing consiste nel manipolare il dataset in modo da eliminare le informazioni inutili o peggio fuorvianti per la rete, oltre all'eliminazione dei dati quel che abbiamo fatto √® andare ad aggiungere informazioni (es. POS Tag) e semplificare le varie frasi (con la lemmatizazione)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulizia dei dati\n",
    "La pulizia dei dati si √® divisa in vari sottopassaggi.  \n",
    "Avendo come dominio il linguaggio naturale dobbiamo tenere conto di tutte le possibili variazioni che possono creare del rumore all'interno del dataset.\n",
    "\n",
    "Come prima cosa abbiamo portato tutto il dataset in **caratteri minuscoli**, in quanto le lettere maiuscole sono caratteri diversi che veicolano le stesse informazioni di quelli minuscoli.\n",
    "\n",
    "Dopo di che abbiamo fatto un primo passaggio di standardizzazione del linguaggio **eliminando tutte le contrazioni** presenti in lingua inglese e ponendole in forma estesa (es. \"*you're*\" -> \"*you are*\").\n",
    "\n",
    "Insieme all'eliminazione delle contrazioni abbiamo fatto la **pulizia dello slang** solito dell'internet, andando a sostituire tutte quelle sigle, forme contratte ed abbreviazioni con la loro controparte \"canonica\" (es. \"*m8*\" -> \"*mate*\").\n",
    "\n",
    "Dopo di che si √® passati all'**eliminazione di tutti i caratteri speciali**, in particolare parliamo di simboli, link, tag HTML, caratteri non ASCII.  \n",
    "Si √® deciso quindi di eliminare anche la punteggiatura.\n",
    "\n",
    "Infine √® giunto il momento di **togliere le stopwords**, cio√© tutte quelle parole di circostanza che aiutano nella forma ma non veicolano nessuna informazione utile (es. \"*the*\", \"*and*\" etc.)\n",
    "\n",
    "Una volta fatto ci√≤ sono stati **eliminati quegli ultimi tag rimasti** (```\\r``` e ```\\n```), **i caratteri di spazio ridondanti e quelli ad inizio e fine riga**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "stop = set(stopwords.words('english'))\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def cleaning(text):\n",
    "\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove Contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove Slangs\n",
    "    text = slang_clean(text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "\n",
    "    # Remove links\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove html tags\n",
    "    text = re.sub ('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', text)\n",
    "\n",
    "    # Remove non ASCII characters\n",
    "    text = text.encode(\"ascii\", \"ignore\")\n",
    "    text = text.decode()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "    # Remove words with numbers in them\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Spellings correction\n",
    "    #text = TextBlob(text).correct()\n",
    "\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop)\n",
    "    \n",
    "    # Remove \\n and \\r\n",
    "    text = re.sub(r'(\\n)+', ' ', text)\n",
    "    text = re.sub(r'(\\r)+', '', text)\n",
    "\n",
    "    # Remove starting and ending spaces\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def slang_clean(text):\n",
    "        \"\"\"\n",
    "            Other manual text cleaning techniques\n",
    "        \"\"\"\n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"‚Ç¨\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l2p\" : \"learn to play\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"stfu\" : \"shut the fuck up\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Cleaning\n",
    "train_data['text_clean'] = train_data['comment_text'].apply(lambda x: cleaning(x))\n",
    "val_df['text_clean'] = val_df['comment_text'].apply(lambda x: cleaning(x))\n",
    "test_data['text_clean'] = test_data['comment_text'].apply(lambda x: cleaning(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìë Part of Speech Tagging & Lemmatizzazione**\n",
    "Successivamente alla pulizia del testo si pu√≤ procedere al **Part of Speech Tagging**, cio√® quell'operazione che associa ad ogni parola un tag tra i seguenti:\n",
    "- N: noun (nome)\n",
    "- V: verb (verbo)\n",
    "- J: adj (aggettivo)\n",
    "- R: adv (avverbio)\n",
    "\n",
    "Questo permette all'operazione successiva, la **lemmatizzazione**, di avvenire in maniera migliore.\n",
    "\n",
    "La lemmatizzazione √® quell'operazione che porta tutti i sostantivi alla forma base, per esempio i verbi vengono tutti portati all'infinito e gli aggettivi vengono portati tutti alla forma base, andando a modificare eventuali superlativi etc.  \n",
    "Abbiamo scelto di effettuare la lemmatizzazione anzich√© solo uno stemming in quanto abbiamo valutato che, per i nostri scopi, informazioni come il tempo verbale non fossero rilevanti, al contratio l'utilizzo di pi√π parole per veicolare lo stesso messaggio avrebbe solo aggiunto rumore al nostro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('brown')\n",
    "\n",
    "\n",
    "# Part of Speech Tagging\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
    "    \"\"\"\n",
    "        Create pos_tag with wordnet format\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    # map the pos tagging output with wordnet output \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "   \n",
    "    return pos_tagged_text\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize_word(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma\n",
    "\n",
    "# Apply Pos Tagging\n",
    "train_data['separated'] = train_data['text_clean'].apply(lambda x: [x for x in x.split()])\n",
    "train_data['text_pos'] = train_data['separated'].apply(lambda x: pos_tag_wordnet(x)) \n",
    "val_df['separated'] = val_df['text_clean'].apply(lambda x: [x for x in x.split()])\n",
    "val_df['text_pos'] = val_df['separated'].apply(lambda x: pos_tag_wordnet(x))\n",
    "test_data['separated'] = test_data['text_clean'].apply(lambda x: [x for x in x.split()])\n",
    "test_data['text_pos'] = test_data['separated'].apply(lambda x: pos_tag_wordnet(x))\n",
    "\n",
    "# Apply Lemmatization\n",
    "train_data['text_lem_wpos'] = train_data['text_pos'].apply(lambda x: lemmatize_word(x))\n",
    "train_data['text_lem'] = [' '.join(map(str,l)) for l in train_data['text_lem_wpos']]\n",
    "val_df['text_lem_wpos'] = val_df['text_pos'].apply(lambda x: lemmatize_word(x))\n",
    "val_df['text_lem'] = [' '.join(map(str,l)) for l in val_df['text_lem_wpos']]\n",
    "test_data['text_lem_wpos'] = test_data['text_pos'].apply(lambda x: lemmatize_word(x))\n",
    "test_data['text_lem'] = [' '.join(map(str,l)) for l in test_data['text_lem_wpos']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis (Pt.2)\n",
    "Dopo un primo preprocessing possiamo effettuare una seconda analisi dei dati, andando a creare delle wordcloud per l'intero dataset e per ogni singola classe di tossicit√†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "mask = np.array(Image.open('./images/wikipedia_mask.jpg'))\n",
    "\n",
    "def generate_wordcloud(df, clm):\n",
    "    text = []\n",
    "    comments = train_data.loc[df[clm] == 1]['text_clean']\n",
    "\n",
    "    for c in comments:\n",
    "        text.append(c) \n",
    "    words = ' '.join(text)\n",
    "    return WordCloud(stopwords=stop, background_color='white', mask=mask, height=1500, width=1500).generate(words)\n",
    "\n",
    "train_toxic = generate_wordcloud(train_data, 'toxic')\n",
    "train_sev_toxic = generate_wordcloud(train_data, 'severe_toxic')\n",
    "train_obscene = generate_wordcloud(train_data, 'obscene')\n",
    "train_threat = generate_wordcloud(train_data, 'threat')\n",
    "train_insult = generate_wordcloud(train_data, 'insult')\n",
    "train_id_hate = generate_wordcloud(train_data, 'identity_hate')\n",
    "train_general = WordCloud(stopwords=stop, background_color='white', height=1500, width=4500).generate(\" \".join(train['text_clean']))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(45, 10), gridspec_kw = {'wspace':0.01, 'hspace':0.1})\n",
    "axes.imshow(train_general)\n",
    "axes.axis('off')\n",
    "axes.set_title('General Word Cloud')\n",
    "\n",
    "plt.show()\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10), gridspec_kw = {'wspace':0.01, 'hspace':0.1})\n",
    "\n",
    "\n",
    "axes[0][0].imshow(train_toxic)\n",
    "axes[0][0].axis('off')\n",
    "axes[0][0].set_title('Toxic Word Cloud')\n",
    "axes[0][0].set_aspect('equal')\n",
    "\n",
    "axes[0][1].imshow(train_sev_toxic)\n",
    "axes[0][1].axis('off')\n",
    "axes[0][1].set_title('Severely Toxic Word Cloud')\n",
    "axes[0][1].set_aspect('equal')\n",
    "\n",
    "axes[0][2].imshow(train_obscene)\n",
    "axes[0][2].axis('off')\n",
    "axes[0][2].set_title('Obscene Word Cloud')\n",
    "axes[0][2].set_aspect('equal')\n",
    "\n",
    "axes[1][0].imshow(train_threat)\n",
    "axes[1][0].axis('off')\n",
    "axes[1][0].set_title('Threat Word Cloud')\n",
    "axes[1][0].set_aspect('equal')\n",
    "\n",
    "axes[1][1].imshow(train_insult)\n",
    "axes[1][1].axis('off')\n",
    "axes[1][1].set_title('Insult Word Cloud')\n",
    "axes[1][1].set_aspect('equal')\n",
    "\n",
    "axes[1][2].imshow(train_id_hate)\n",
    "axes[1][2].axis('off')\n",
    "axes[1][2].set_title('Identity Hate Word Cloud')\n",
    "axes[1][2].set_aspect('equal')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio del dataset pulito\n",
    "Commentato per non creare problemi nel momento in cui si vuole eseguire il notebook da zero eseguendo tutte le celle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['text_lem', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "#train_data.to_csv('data/train_clean.csv', columns = header, index = False)\n",
    "#test_wlabels.to_csv('data/test_clean.csv', columns = header, index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dati gi√† puliti\n",
    "Commentato per non creare problemi nel momento in cui si vuole eseguire il notebook da zero eseguendo tutte le celle.  \n",
    "Nel caso si voglia caricare il dataset gi√† pulito, basta decommentare la cella e far partire l'esecuzione dalla cella sottostante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = pd.read_csv('data/train_clean.csv')\n",
    "#test_data = pd.read_csv('data/test_clean.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PyTorch e BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In presenza di GPU Nvidia si sfrutta l'accellerazione CUDA, altrimenti si utilizza la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ottimizzatore scelto √® AdamW, una versione dell'ottimizzatore Adam. Nello specifico Adam (ADaptive Moment Estimation) combina l'idea del momentum con quella del Root Mean Squared Prop (RMSProp) e dunque permette di avere il momentum nelle direzioni dove il gradiente √® sempre lo stesso e di smorzarlo quando fluttua in presenza di varianza. In generale ci si sta spostando verso versioni di Adam, come AdamW, in quanto riducono il rischio di overfitting.\n",
    "Ad AdamW sono stati passati gli iperparametri di BERT e sono stati impostati un *learning rate* pari a *1<sup>-4<sup>* e un *epsilon* pari a *1<sup>-8<sup>*.\n",
    "\n",
    "Il training √® stato fatto su 5 epoche, con una batch size di 16 e *BCE with Logits* come funzione loss. \n",
    "\n",
    ">Gli autori di BERT suggeriscono di utilizzare un batch size di 16 o 32, ma in questo caso abbiamo scelto 16 per evitare di esaurire la memoria GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "# PreProcessing for bert\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def preprocessing_for_bert(data):\n",
    "    \n",
    "    '''\n",
    "    Add special tokens to the start and end of each sentence.\n",
    "    Pad & truncate all sentences to a single constant length.\n",
    "    Explicitly differentiate real tokens from padding tokens with the ‚Äúattention mask‚Äù.\n",
    "\n",
    "    '''\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text = sent,\n",
    "            add_special_tokens = True,\n",
    "            max_length = 300,\n",
    "            pad_to_max_length = True,\n",
    "            truncation = True,\n",
    "            return_attention_mask = True\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "    \n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 6\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        # self.LSTM = nn.LSTM(D_in,D_in,bidirectional=True)\n",
    "        # self.clf = nn.Linear(D_in*2,2)\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(nn.Linear(D_in, H), nn.Dropout(0.3), nn.Linear(H, D_out))\n",
    "        \n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        \n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]        \n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=1e-4,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    best_accuracy = 0\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "        step = 0\n",
    "        # For each batch of training data...\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "            b_input_ids = b_input_ids.to(device,dtype=torch.long)\n",
    "            b_attn_mask = b_attn_mask.to(device,dtype=torch.long)\n",
    "            b_labels = b_labels.to(device,dtype=torch.float)\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            logits = torch.sigmoid(logits)\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                #print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "                \n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "           \n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "        step += 1\n",
    "        output_folder = \"bert_classifier_multilabel\"\n",
    "        filename=\"./trained_models/\"+str(output_folder)\n",
    "\n",
    "        # create folder if it does not exist\n",
    "\n",
    "        if not os.path.exists(filename):\n",
    "            os.makedirs(filename)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            filename=\"./trained_models/\"+str(output_folder)+\"/best.pt\"\n",
    "            torch.save(model.state_dict(), filename)\n",
    "\n",
    "        filename=\"./trained_models/\"+str(output_folder)+\"/last.pt\"\n",
    "        torch.save(model.state_dict(), filename)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        logits = torch.sigmoid(logits)\n",
    "        loss = loss_fn(logits, b_labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = logits\n",
    "        # Calculate the accuracy rate\n",
    "        correct_val = 0\n",
    "        res = 0\n",
    "        for i in range(6):\n",
    "            res = 1 if preds[0,i]>0.5 else 0\n",
    "            if res == b_labels[0,i]:\n",
    "                correct_val += 1\n",
    "        val_accuracy.append(correct_val/6)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "#Splitting\n",
    "\n",
    "train_inputs, train_masks = preprocessing_for_bert(train_df['text_lem'].tolist())\n",
    "val_inputs, val_masks = preprocessing_for_bert(val_df['text_lem'].tolist())\n",
    "\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(train_df[labels].to_numpy())\n",
    "val_labels = torch.tensor(val_df[labels].to_numpy())\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "# Create the DataLoader for our training set\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Specify loss function\n",
    "#loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulizia della memoria allocata nella GPU\n",
    "In caso di esecuzione su GPU, questa cella serve per liberare la memoria allocata dalla GPU.\n",
    "Nel caso l'esecuzione non riesca a liberare la mamoria √® consigliato chiudere e riaprire il notebook (e Visual Studio Code nel caso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=5, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "    labels = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(torch.sigmoid(logits).cpu().detach().numpy().tolist())\n",
    "    # Concatenate logits from each batch\n",
    "    \n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "    return all_logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione e risultati\n",
    "L'**accuratezza** √® una metrica comune per valutare la performance di un modello di classificazione. Si calcola come il numero di predizioni corrette su totale di predizioni fatte. In una classificazione multilabel, l'accuratezza √® il rapporto tra il numero di etichette predette correttamente e il numero totale di etichette predette.  \n",
    "\n",
    "L'**hamming score**, invece, si concentra sul numero di etichette predette correttamente su un numero totale di etichette possibili, quindi risulta una metrica attendibile nelle classificazioni multi-label.\n",
    "In altre parole, l'hamming score misura il grado di somiglianza tra le etichette predette e quelle effettive, un punteggio pi√π alto indica che le etichette predette sono pi√π simili a quelle effettive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Se si vuole caricare il miglior modello gi√† allenato, basta decommentare la cella e far partire l'esecuzione dalla cella sottostante.\n",
    ">Se non si commenta la cella verr√† utilizzato il modello allenato in precedenza pi√π recente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model = BertClassifier()\n",
    "#best_model.load_state_dict(torch.load('trained_models/bert_classifier_multilabel/best.pt'))\n",
    "#best_model.to(device)y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs, test_masks = preprocessing_for_bert(test_data['text_lem'].tolist())\n",
    "test_y = torch.tensor(test_data[labels].to_numpy())\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_y)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=16)\n",
    "\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    \"\"\"Compute the average Hamming score (a.k.a. label-based accuracy) for the multi-label case\"\"\"\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for p in range(len(probs)):\n",
    "    for b in range(len(probs[p])):\n",
    "        aux = []\n",
    "        for l in range(6):\n",
    "            aux.append(int(probs[p][b][l] >= 0.5))\n",
    "        outputs.append(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "test_label = test_data[labels].to_numpy()\n",
    "for p in range(len(outputs)):\n",
    "    correct_val = 0\n",
    "    for i in range(6):\n",
    "        if outputs[p][i] == test_label[p,i]:\n",
    "            correct_val += 1\n",
    "    accuracy.append(correct_val/6)\n",
    "print(f'Accuracy: {np.mean(accuracy)}')\n",
    "\n",
    "hammingscore = hamming_score(test_label, outputs)\n",
    "print(\"Hamming score: \", hammingscore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_sentence(sentence):\n",
    "    sentence = cleaning(sentence)\n",
    "    sentence, sentence_mask = preprocessing_for_bert([sentence])\n",
    "\n",
    "    sentence_dataset = TensorDataset(sentence, sentence_mask)\n",
    "    sentence_sampler = SequentialSampler(sentence_dataset)\n",
    "    sentence_dataloader = DataLoader(sentence_dataset, sampler=sentence_sampler, batch_size=1)\n",
    "    prob_sentence = bert_predict(bert_classifier,sentence_dataloader)\n",
    "    outputs = []\n",
    "    for p in range(len(prob_sentence)):\n",
    "        for b in range(len(prob_sentence[p])):\n",
    "            aux = []\n",
    "            for l in range(6):\n",
    "                aux.append(int(prob_sentence[p][b][l] >= 0.5))\n",
    "            outputs.append(aux)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"yellow submarine\"\n",
    "prediction = predictions_sentence(sentence)[0]\n",
    "\n",
    "print(f'toxic: {prediction[0]}, severe toxic: {prediction[1]}, obscene: {prediction[2]}, threat: {prediction[3]}, insult:{prediction[4]}, identity hate: {prediction[5]}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Sep 26 2022, 11:37:49) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d824d8a0e2d11e6a63b0c00015cf64289480a99dfec38a3fa91fac875798931a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
