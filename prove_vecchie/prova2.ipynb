{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import metrics as kmetrics\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import sklearn.metrics as metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    return data\n",
    "\n",
    "def clean_text(text):\n",
    "    \" Clean the text from special characters, links, punctuation and words with numbers in them \"\n",
    "    \n",
    "    # Modify text in lower case\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Delete special characters\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Delete links\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Delete punctuation\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    #text = re.sub('\\n', '', text)\n",
    "    \n",
    "    # Delete words with numbers in them\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "def preprocessing_data(text):\n",
    "    \" Preprocess the data to be more readable for the model\"\n",
    "    \n",
    "    # Download stopwords if not downloaded\n",
    "    if stopwords is None:\n",
    "        nltk.download('stopwords')\n",
    "     \n",
    "    # Clean text   \n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    more_stopwords = ['u', 'im', 'c']\n",
    "    stop_words = stop_words + more_stopwords\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "\n",
    "    # Stemming all words\n",
    "    #stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    #text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv files and save them in dataframes\n",
    "    \n",
    "# labels = id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "train = read_csv_file('data/train.csv')\n",
    "   \n",
    "# labels = id, comment_text\n",
    "test = read_csv_file('data/test.csv')\n",
    "    \n",
    "# labels = id, toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "test_labels = read_csv_file('data/test_labels copy.csv')\n",
    "\n",
    "embedding_file='data/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "train['comment_text_clean'] = train['comment_text'].apply(preprocessing_data)\n",
    "test['comment_text_clean'] = test['comment_text'].apply(preprocessing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text_clean'].fillna(\"unknown\", inplace=True)\n",
    "test['comment_text_clean'].fillna(\"unknown\", inplace=True)\n",
    "\n",
    "x_train = train['comment_text_clean']\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train[list_classes].values\n",
    "    \n",
    "x_test = test['comment_text_clean']\n",
    "y_test = test_labels[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words to use (i.e num rows in embedding vector)\n",
    "max_features = 20000\n",
    "# Max number of words in each comment\n",
    "maxlen = 100\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_training = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "x_testing = pad_sequences(list_tokenized_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessandraperniciano/Github/DLA-sentiment-analysis-wikipedia-comments/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3373: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "# Read the glove word vectors (space delimited strings) into a dictionary from word->vector.\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))\n",
    "\n",
    "# Use these vectors to create our embedding matrix,\n",
    "# with random initialization for words that aren't in GloVe.\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "embed_size = 50 # how big is each word vector\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM with two fully connected layers\n",
    "\n",
    "inp = Input(shape=(maxlen,))\n",
    "\n",
    "# Embedding layer, 20000 is the size of the vocabulary, 50 is the dimensionality of the output space\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "\n",
    "# LSTM is a type of RNN, 50 is the dimensionality of the output space\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x) \n",
    "\n",
    "# Global max pooling\n",
    "x = GlobalMaxPool1D()(x) \n",
    "\n",
    "# Fully connected layer\n",
    "x = Dense(50, activation=\"relu\")(x) \n",
    "\n",
    "# Dropout to avoid overfitting\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# 6 output nodes, for 6 categories\n",
    "x = Dense(6, activation=\"sigmoid\")(x) \n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4488/4488 [==============================] - 754s 168ms/step - loss: 0.0348 - accuracy: 0.9375 - val_loss: 0.0498 - val_accuracy: 0.9912\n",
      "Epoch 2/2\n",
      "4488/4488 [==============================] - 685s 153ms/step - loss: 0.0321 - accuracy: 0.9015 - val_loss: 0.0555 - val_accuracy: 0.9898\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_training, y_train, batch_size=32, epochs=2, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "1197/1197 [==============================] - 47s 39ms/step - loss: 6.5820 - accuracy: 0.9891\n",
      "test loss, test acc: [6.582034587860107, 0.9890509247779846]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_testing, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.03478873893618584, 0.03211464360356331],\n",
       " 'accuracy': [0.9374847412109375, 0.9014643430709839],\n",
       " 'val_loss': [0.049757570028305054, 0.055527154356241226],\n",
       " 'val_accuracy': [0.9911643266677856, 0.9897856712341309]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction_train = model.predict([x_training], batch_size=1024, verbose=1)\n",
    "#prediction_test = model.predict([x_testing], batch_size=1024, verbose=1)\n",
    "history.history\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "#def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "#n = train.shape[0]\n",
    "#vec = <TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "#               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "#               smooth_idf=1, sublinear_tf=1 )\n",
    "#trn_term_doc = vec.fit_transform(train['comment_text_clean'])\n",
    "#test_term_doc = vec.transform(test['comment_text_clean'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "pipe = Pipeline([\n",
    "        ('bow', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('model', xgb.XGBClassifier(\n",
    "            use_label_encoder=False, \n",
    "            eval_metric='auc',\n",
    "            ))\n",
    "    ])\n",
    "    \n",
    "print(\"------------ Fit the model ------------ \\n\")\n",
    "pipe.fit(x_train, y_train)\n",
    "    \n",
    "y_pred_class = pipe.predict(x_test)\n",
    "y_pred_train = pipe.predict(x_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print(\"------------ Results ------------ \\n\")\n",
    "print('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\n",
    "print('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))\n",
    "\n",
    "matrix = metrics.confusion_matrix(y_test, y_pred_class, labels = pipe.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=pipe.classes_)\n",
    "color = 'white'\n",
    "disp.plot()\n",
    "plt.xlabel('Predicted Label', color=color)\n",
    "plt.ylabel('True Label', color=color)\n",
    "plt.gcf().axes[0].tick_params(colors=color)\n",
    "plt.gcf().axes[1].tick_params(colors=color)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "43f0fe85b3275fa0ff4a40da0fbb6ff13233afb674851cc1511fc5f060fdb3fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
