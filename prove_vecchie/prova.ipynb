{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing ft2font: Risorse di memoria disponibili insufficienti per elaborare il comando.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m stopwords\n",
      "File \u001b[1;32mc:\\Users\\aless\\anaconda3\\envs\\DLA\\lib\\site-packages\\matplotlib\\__init__.py:207\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[39mif\u001b[39;00m LooseVersion(module\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m minver:\n\u001b[0;32m    203\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMatplotlib requires \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m>=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m; you have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m                               \u001b[39m.\u001b[39mformat(modname, minver, module\u001b[39m.\u001b[39m__version__))\n\u001b[1;32m--> 207\u001b[0m _check_versions()\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(sys, \u001b[39m'\u001b[39m\u001b[39margv\u001b[39m\u001b[39m'\u001b[39m):  \u001b[39m# for modpython\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     sys\u001b[39m.\u001b[39margv \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmodpython\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\aless\\anaconda3\\envs\\DLA\\lib\\site-packages\\matplotlib\\__init__.py:192\u001b[0m, in \u001b[0;36m_check_versions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_versions\u001b[39m():\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m     \u001b[39m# Quickfix to ensure Microsoft Visual C++ redistributable\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     \u001b[39m# DLLs are loaded before importing kiwisolver\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m ft2font\n\u001b[0;32m    194\u001b[0m     \u001b[39mfor\u001b[39;00m modname, minver \u001b[39min\u001b[39;00m [\n\u001b[0;32m    195\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39mcycler\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0.10\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    196\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39mdateutil\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2.1\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m             (\u001b[39m\"\u001b[39m\u001b[39mpyparsing\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2.0.1\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m    200\u001b[0m     ]:\n\u001b[0;32m    201\u001b[0m         module \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(modname)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing ft2font: Risorse di memoria disponibili insufficienti per elaborare il comando."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from transformers import TFBertModel\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'ur', 'dont', 'doin', 'ure']\n",
    "stop_words = stop_words + more_stopwords\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(file_name):\n",
    "    return pd.read_csv(file_name)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove text in square brackets\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove links\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "    # Remove words containing numbers\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "def stem_words(text):\n",
    "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
    "\n",
    "def preprocess_data(text):\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = stem_words(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv files and save them in dataframes\n",
    "    \n",
    "# labels = id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "train = read_csv_file('data/train.csv')\n",
    "   \n",
    "# labels = id, comment_text\n",
    "test = read_csv_file('data/test.csv')\n",
    "    \n",
    "# labels = id, toxic, severe_toxic, obscene, threat, insult, identity_hate\n",
    "test_labels = read_csv_file('data/test_labels copy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "train['text_clean'] = train['comment_text'].apply(lambda x: preprocess_data(x))\n",
    "test['text_clean'] = test['comment_text'].apply(lambda x: preprocess_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train['text_clean']\n",
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "\n",
    "x_test = test['text_clean']\n",
    "y_test = test_labels[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorazing the text\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(x_train)\n",
    "\n",
    "#Document Term Matrix from train and test_sets\n",
    "x_train_dtm = vect.transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_tunned = CountVectorizer(stop_words = 'english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_transformer.fit(x_train_dtm)\n",
    "x_train_tfidf = tfidf_transformer.transform(x_train_dtm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_tokenizer \u001b[39m=\u001b[39m Tokenizer()\n\u001b[0;32m      2\u001b[0m word_tokenizer\u001b[39m.\u001b[39mfit_on_texts(x_train)\n\u001b[0;32m      4\u001b[0m vocab_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(word_tokenizer\u001b[39m.\u001b[39mword_index) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "#NON NECESSARIO CON BERT\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m longest_train \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(x, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m sentence: \u001b[39mlen\u001b[39m(word_tokenize(sentence)))\n\u001b[0;32m      7\u001b[0m length_long_sentence \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(word_tokenize(longest_train))\n\u001b[0;32m      9\u001b[0m train_padded_sentences \u001b[39m=\u001b[39m pad_sequences(\n\u001b[1;32m---> 10\u001b[0m     embed(x), length_long_sentence, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m, in \u001b[0;36membed\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(corpus): \n\u001b[1;32m----> 3\u001b[0m     \u001b[39mreturn\u001b[39;00m word_tokenizer\u001b[39m.\u001b[39mtexts_to_sequences(corpus)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "x = train['text_clean']\n",
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "\n",
    "longest_train = max(x, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "\n",
    "train_padded_sentences = pad_sequences(\n",
    "    embed(x), length_long_sentence, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_dictionary = dict()\n",
    "embedding_dim = 50\n",
    "\n",
    "with open('data/glove.6B.50d.txt', encoding=\"utf8\") as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dictionary[word] = vector_dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Now we will load embedding vectors of those words that appear in the\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Glove dictionary. Others will be initialized to 0.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m embedding_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((vocab_length, embedding_dim))\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m word, index \u001b[39min\u001b[39;00m word_tokenizer\u001b[39m.\u001b[39mword_index\u001b[39m.\u001b[39mitems():\n\u001b[0;32m      7\u001b[0m     embedding_vector \u001b[39m=\u001b[39m embeddings_dictionary\u001b[39m.\u001b[39mget(word)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_length' is not defined"
     ]
    }
   ],
   "source": [
    "# Now we will load embedding vectors of those words that appear in the\n",
    "# Glove dictionary. Others will be initialized to 0.\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231508/231508 [00:00<00:00, 622318.61B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def bert_encode(data, maximum_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in tqdm(data):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text=text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=maximum_length,\n",
    "            padding=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids), np.array(attention_masks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_input_ids, train_attention_masks \u001b[39m=\u001b[39m bert_encode(x_train, \u001b[39m60\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_input_ids, train_attention_masks = bert_encode(x_train, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert):\n",
    "    input_ids = Input(shape=(60,), dtype=tf.int32)\n",
    "    attention_masks = Input(shape=(60,), dtype=tf.int32)\n",
    "    output = bert([input_ids, attention_masks])\n",
    "    output = output[1]\n",
    "    output = Dense(32, activation='relu')(output)\n",
    "    output = Dropout(0.2)(output)\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "    model = Model(inputs=[input_ids, attention_masks], outputs=output)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m create_model(bert_model)\n\u001b[0;32m      2\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(bert)\u001b[0m\n\u001b[0;32m      2\u001b[0m input_ids \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39m60\u001b[39m,), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m      3\u001b[0m attention_masks \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(\u001b[39m60\u001b[39m,), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[1;32m----> 4\u001b[0m output \u001b[39m=\u001b[39m bert([input_ids, attention_masks])\n\u001b[0;32m      5\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m output \u001b[39m=\u001b[39m Dense(\u001b[39m32\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(output)\n",
      "File \u001b[1;32mc:\\Users\\aless\\anaconda3\\envs\\DLA\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aless\\anaconda3\\envs\\DLA\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:962\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    961\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 962\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[0;32m    963\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "model = create_model(bert_model)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 | packaged by conda-forge | (default, Nov 22 2019, 19:04:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c39809148757edcb3b6daffabc169aba51e31ec3dbc492037eae5bf13bbe6f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
