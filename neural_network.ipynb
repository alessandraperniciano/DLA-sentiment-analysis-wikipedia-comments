{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import string\n",
    "import re\n",
    "import io\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dei dati\n",
    "Il dataset di partenza √® stato preso da una [challenge](https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/) pubblicata su [Kaggle](https://www.kaggle.com/) nel 2018 da [Jigsaw](https://jigsaw.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data with labels\n",
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "test_wlabels = test.merge(test_labels)\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for s in labels:\n",
    "    test_wlabels = test_wlabels[test_wlabels[s] != -1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis (pt.1) e subsampling\n",
    "Il dataset risulta essere molto sbilanciato, per questo motivo √® stato optato per un subsampling della classe dominante, che risulta essere quella con tutte le classi poste a zero.  \n",
    "Si √® scelto di utilizzare la proporzione 50/50, dove met√† del dataset risultano essere commenti \"puliti\", mentre l'altra met√† dei record ha un qualche grado di tossicit√†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_ppl_counter(data):\n",
    "    count = 0\n",
    "    for d in range(len(data)):\n",
    "        if data['toxic'][d] == 0 and data['severe_toxic'][d] == 0 and data['obscene'][d] == 0 and data['threat'][d] == 0 and data['insult'][d] == 0 and data['identity_hate'][d] == 0:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def bad_ppl_counter(data):\n",
    "    count = 0\n",
    "    for d in range(len(data)):\n",
    "        for s in labels:\n",
    "            if data[s][d] == 1:\n",
    "                count += 1\n",
    "                break\n",
    "    return count\n",
    "\n",
    "# Length of the train data\n",
    "print(f'Lenght of train data: {len(train_data)}')\n",
    "print(f'Good people: {good_ppl_counter(train_data)}')\n",
    "print(f'Bad people: {bad_ppl_counter(train_data)}')\n",
    "print('..........')\n",
    "\n",
    "# Record count for each label\n",
    "for s in labels:\n",
    "    print(f'{s}: {len(train_data[train_data[s] == 1])}')\n",
    "print('..........')\n",
    "\n",
    "# Record count for each label with toxic = 0\n",
    "count = {}\n",
    "for s in labels:\n",
    "    count[s] = 0\n",
    "for d in range(len(train_data)):\n",
    "    for s in labels:\n",
    "        if train_data['toxic'][d] == 0 and train_data[s][d] == 1:\n",
    "            count[s] += 1\n",
    "for k in count.keys():\n",
    "    print(f'{k}: {count[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train data into train and validation and balance the data\n",
    "train_df, val_df = train_test_split(train, test_size=0.1)\n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_toxic = train_df[train_df[labels].sum(axis=1) > 0]\n",
    "train_clean = train_df[train_df[labels].sum(axis=1) == 0]\n",
    "\n",
    "train_data = pd.concat([\n",
    "  train_toxic,\n",
    "  train_clean.sample(20000)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete from dataframe the rows with nan values\n",
    "test_data = test_data[~test_data['text_lem'].isna()]\n",
    "test_data['text_lem'].isna().sum()\n",
    "train_data = train_data[~train_data['text_lem'].isna()]\n",
    "train_data['text_lem'].isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulizia dei dati\n",
    "La pulizia dei dati si √® divisa in vari sottopassaggi.  \n",
    "Avendo come dominio il linguaggio naturale dobbiamo tenere conto di tutte le possibili variazioni che possono creare del rumore all'interno del dataset.\n",
    "\n",
    "Come prima cosa abbiamo portato tutto il dataset in **caratteri minuscoli**, in quanto le lettere maiuscole sono caratteri diversi che veicolano le stesse informazioni di quelli minuscoli.\n",
    "\n",
    "Dopo di che abbiamo fatto un primo passaggio di standardizzazione del linguaggio **eliminando tutte le contrazioni** presenti in lingua inglese e ponendole in forma estesa (es. \"*you're*\" -> \"*you are*\").\n",
    "\n",
    "Insieme all'eliminazione delle contrazioni abbiamo fatto la **pulizia dello slang** solito dell'internet, andando a sostituire tutte quelle sigle, forme contratte ed abbreviazioni con la loro controparte \"canonica\" (es. \"*m8*\" -> \"*mate*\").\n",
    "\n",
    "Dopo di che si √® passati all'**eliminazione di tutti i caratteri speciali**, in particolare parliamo di simboli, link, tag HTML, caratteri non ASCII.  \n",
    "Si √® deciso quindi di eliminare anche la punteggiatura.\n",
    "\n",
    "Infine √® giunto il momento di **togliere le stopwords**, cio√© tutte quelle parole di circostanza che aiutano nella forma ma non veicolano nessuna informazione utile (es. \"*the*\", \"*and*\" etc.)\n",
    "\n",
    "Una volta fatto ci√≤ sono stati **eliminati quegli ultimi tag rimasti** (```\\r``` e ```\\n```), **i caratteri di spazio ridondanti e quelli ad inizio e fine riga**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import contractions\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def cleaning(text):\n",
    "\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove Contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "\n",
    "    # Remove links\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove html tags\n",
    "    text = re.sub ('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', text)\n",
    "\n",
    "    # Remove non ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "\n",
    "    # Remove words with numbers in them\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Remove Slangs\n",
    "    text = slang_clean(text)\n",
    "\n",
    "    # Spellings correction\n",
    "    #text = TextBlob(text).correct()\n",
    "\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop)\n",
    "    \n",
    "    # Remove \\n and \\r\n",
    "    text = re.sub(r'(\\n)+', ' ', text)\n",
    "    text = re.sub(r'(\\r)+', '', text)\n",
    "\n",
    "    # Remove starting and ending spaces\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def slang_clean(text):\n",
    "        \"\"\"\n",
    "            Other manual text cleaning techniques\n",
    "        \"\"\"\n",
    "        # Typos, slang and other\n",
    "        sample_typos_slang = {\n",
    "                                \"w/e\": \"whatever\",\n",
    "                                \"usagov\": \"usa government\",\n",
    "                                \"recentlu\": \"recently\",\n",
    "                                \"ph0tos\": \"photos\",\n",
    "                                \"amirite\": \"am i right\",\n",
    "                                \"exp0sed\": \"exposed\",\n",
    "                                \"<3\": \"love\",\n",
    "                                \"luv\": \"love\",\n",
    "                                \"amageddon\": \"armageddon\",\n",
    "                                \"trfc\": \"traffic\",\n",
    "                                \"16yr\": \"16 year\"\n",
    "                                }\n",
    "\n",
    "        # Acronyms\n",
    "        sample_acronyms =  { \n",
    "                            \"mh370\": \"malaysia airlines flight 370\",\n",
    "                            \"okwx\": \"oklahoma city weather\",\n",
    "                            \"arwx\": \"arkansas weather\",    \n",
    "                            \"gawx\": \"georgia weather\",  \n",
    "                            \"scwx\": \"south carolina weather\",  \n",
    "                            \"cawx\": \"california weather\",\n",
    "                            \"tnwx\": \"tennessee weather\",\n",
    "                            \"azwx\": \"arizona weather\",  \n",
    "                            \"alwx\": \"alabama weather\",\n",
    "                            \"usnwsgov\": \"united states national weather service\",\n",
    "                            \"2mw\": \"tomorrow\"\n",
    "                            }\n",
    "\n",
    "        \n",
    "        # Some common abbreviations \n",
    "        sample_abbr = {\n",
    "                        \"$\" : \" dollar \",\n",
    "                        \"‚Ç¨\" : \" euro \",\n",
    "                        \"4ao\" : \"for adults only\",\n",
    "                        \"a.m\" : \"before midday\",\n",
    "                        \"a3\" : \"anytime anywhere anyplace\",\n",
    "                        \"aamof\" : \"as a matter of fact\",\n",
    "                        \"acct\" : \"account\",\n",
    "                        \"adih\" : \"another day in hell\",\n",
    "                        \"afaic\" : \"as far as i am concerned\",\n",
    "                        \"afaict\" : \"as far as i can tell\",\n",
    "                        \"afaik\" : \"as far as i know\",\n",
    "                        \"afair\" : \"as far as i remember\",\n",
    "                        \"afk\" : \"away from keyboard\",\n",
    "                        \"app\" : \"application\",\n",
    "                        \"approx\" : \"approximately\",\n",
    "                        \"apps\" : \"applications\",\n",
    "                        \"asap\" : \"as soon as possible\",\n",
    "                        \"asl\" : \"age, sex, location\",\n",
    "                        \"atk\" : \"at the keyboard\",\n",
    "                        \"ave.\" : \"avenue\",\n",
    "                        \"aymm\" : \"are you my mother\",\n",
    "                        \"ayor\" : \"at your own risk\", \n",
    "                        \"b&b\" : \"bed and breakfast\",\n",
    "                        \"b+b\" : \"bed and breakfast\",\n",
    "                        \"b.c\" : \"before christ\",\n",
    "                        \"b2b\" : \"business to business\",\n",
    "                        \"b2c\" : \"business to customer\",\n",
    "                        \"b4\" : \"before\",\n",
    "                        \"b4n\" : \"bye for now\",\n",
    "                        \"b@u\" : \"back at you\",\n",
    "                        \"bae\" : \"before anyone else\",\n",
    "                        \"bak\" : \"back at keyboard\",\n",
    "                        \"bbbg\" : \"bye bye be good\",\n",
    "                        \"bbc\" : \"british broadcasting corporation\",\n",
    "                        \"bbias\" : \"be back in a second\",\n",
    "                        \"bbl\" : \"be back later\",\n",
    "                        \"bbs\" : \"be back soon\",\n",
    "                        \"be4\" : \"before\",\n",
    "                        \"bfn\" : \"bye for now\",\n",
    "                        \"blvd\" : \"boulevard\",\n",
    "                        \"bout\" : \"about\",\n",
    "                        \"brb\" : \"be right back\",\n",
    "                        \"bros\" : \"brothers\",\n",
    "                        \"brt\" : \"be right there\",\n",
    "                        \"bsaaw\" : \"big smile and a wink\",\n",
    "                        \"btw\" : \"by the way\",\n",
    "                        \"bwl\" : \"bursting with laughter\",\n",
    "                        \"c/o\" : \"care of\",\n",
    "                        \"cet\" : \"central european time\",\n",
    "                        \"cf\" : \"compare\",\n",
    "                        \"cia\" : \"central intelligence agency\",\n",
    "                        \"csl\" : \"can not stop laughing\",\n",
    "                        \"cu\" : \"see you\",\n",
    "                        \"cul8r\" : \"see you later\",\n",
    "                        \"cv\" : \"curriculum vitae\",\n",
    "                        \"cwot\" : \"complete waste of time\",\n",
    "                        \"cya\" : \"see you\",\n",
    "                        \"cyt\" : \"see you tomorrow\",\n",
    "                        \"dae\" : \"does anyone else\",\n",
    "                        \"dbmib\" : \"do not bother me i am busy\",\n",
    "                        \"diy\" : \"do it yourself\",\n",
    "                        \"dm\" : \"direct message\",\n",
    "                        \"dwh\" : \"during work hours\",\n",
    "                        \"e123\" : \"easy as one two three\",\n",
    "                        \"eet\" : \"eastern european time\",\n",
    "                        \"eg\" : \"example\",\n",
    "                        \"embm\" : \"early morning business meeting\",\n",
    "                        \"encl\" : \"enclosed\",\n",
    "                        \"encl.\" : \"enclosed\",\n",
    "                        \"etc\" : \"and so on\",\n",
    "                        \"faq\" : \"frequently asked questions\",\n",
    "                        \"fawc\" : \"for anyone who cares\",\n",
    "                        \"fb\" : \"facebook\",\n",
    "                        \"fc\" : \"fingers crossed\",\n",
    "                        \"fig\" : \"figure\",\n",
    "                        \"fimh\" : \"forever in my heart\", \n",
    "                        \"ft.\" : \"feet\",\n",
    "                        \"ft\" : \"featuring\",\n",
    "                        \"ftl\" : \"for the loss\",\n",
    "                        \"ftw\" : \"for the win\",\n",
    "                        \"fwiw\" : \"for what it is worth\",\n",
    "                        \"fyi\" : \"for your information\",\n",
    "                        \"g9\" : \"genius\",\n",
    "                        \"gahoy\" : \"get a hold of yourself\",\n",
    "                        \"gal\" : \"get a life\",\n",
    "                        \"gcse\" : \"general certificate of secondary education\",\n",
    "                        \"gfn\" : \"gone for now\",\n",
    "                        \"gg\" : \"good game\",\n",
    "                        \"gl\" : \"good luck\",\n",
    "                        \"glhf\" : \"good luck have fun\",\n",
    "                        \"gmt\" : \"greenwich mean time\",\n",
    "                        \"gmta\" : \"great minds think alike\",\n",
    "                        \"gn\" : \"good night\",\n",
    "                        \"g.o.a.t\" : \"greatest of all time\",\n",
    "                        \"goat\" : \"greatest of all time\",\n",
    "                        \"goi\" : \"get over it\",\n",
    "                        \"gps\" : \"global positioning system\",\n",
    "                        \"gr8\" : \"great\",\n",
    "                        \"gratz\" : \"congratulations\",\n",
    "                        \"gyal\" : \"girl\",\n",
    "                        \"h&c\" : \"hot and cold\",\n",
    "                        \"hp\" : \"horsepower\",\n",
    "                        \"hr\" : \"hour\",\n",
    "                        \"hrh\" : \"his royal highness\",\n",
    "                        \"ht\" : \"height\",\n",
    "                        \"ibrb\" : \"i will be right back\",\n",
    "                        \"ic\" : \"i see\",\n",
    "                        \"icq\" : \"i seek you\",\n",
    "                        \"icymi\" : \"in case you missed it\",\n",
    "                        \"idc\" : \"i do not care\",\n",
    "                        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "                        \"idgaf\" : \"i do not give a fuck\",\n",
    "                        \"idk\" : \"i do not know\",\n",
    "                        \"ie\" : \"that is\",\n",
    "                        \"i.e\" : \"that is\",\n",
    "                        \"ifyp\" : \"i feel your pain\",\n",
    "                        \"IG\" : \"instagram\",\n",
    "                        \"iirc\" : \"if i remember correctly\",\n",
    "                        \"ilu\" : \"i love you\",\n",
    "                        \"ily\" : \"i love you\",\n",
    "                        \"imho\" : \"in my humble opinion\",\n",
    "                        \"imo\" : \"in my opinion\",\n",
    "                        \"imu\" : \"i miss you\",\n",
    "                        \"iow\" : \"in other words\",\n",
    "                        \"irl\" : \"in real life\",\n",
    "                        \"j4f\" : \"just for fun\",\n",
    "                        \"jic\" : \"just in case\",\n",
    "                        \"jk\" : \"just kidding\",\n",
    "                        \"jsyk\" : \"just so you know\",\n",
    "                        \"l8r\" : \"later\",\n",
    "                        \"lb\" : \"pound\",\n",
    "                        \"lbs\" : \"pounds\",\n",
    "                        \"ldr\" : \"long distance relationship\",\n",
    "                        \"lmao\" : \"laugh my ass off\",\n",
    "                        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "                        \"lol\" : \"laughing out loud\",\n",
    "                        \"ltd\" : \"limited\",\n",
    "                        \"ltns\" : \"long time no see\",\n",
    "                        \"m8\" : \"mate\",\n",
    "                        \"mf\" : \"motherfucker\",\n",
    "                        \"mfs\" : \"motherfuckers\",\n",
    "                        \"mfw\" : \"my face when\",\n",
    "                        \"mofo\" : \"motherfucker\",\n",
    "                        \"mph\" : \"miles per hour\",\n",
    "                        \"mr\" : \"mister\",\n",
    "                        \"mrw\" : \"my reaction when\",\n",
    "                        \"ms\" : \"miss\",\n",
    "                        \"mte\" : \"my thoughts exactly\",\n",
    "                        \"nagi\" : \"not a good idea\",\n",
    "                        \"nbc\" : \"national broadcasting company\",\n",
    "                        \"nbd\" : \"not big deal\",\n",
    "                        \"nfs\" : \"not for sale\",\n",
    "                        \"ngl\" : \"not going to lie\",\n",
    "                        \"nhs\" : \"national health service\",\n",
    "                        \"nrn\" : \"no reply necessary\",\n",
    "                        \"nsfl\" : \"not safe for life\",\n",
    "                        \"nsfw\" : \"not safe for work\",\n",
    "                        \"nth\" : \"nice to have\",\n",
    "                        \"nvr\" : \"never\",\n",
    "                        \"nyc\" : \"new york city\",\n",
    "                        \"oc\" : \"original content\",\n",
    "                        \"og\" : \"original\",\n",
    "                        \"ohp\" : \"overhead projector\",\n",
    "                        \"oic\" : \"oh i see\",\n",
    "                        \"omdb\" : \"over my dead body\",\n",
    "                        \"omg\" : \"oh my god\",\n",
    "                        \"omw\" : \"on my way\",\n",
    "                        \"p.a\" : \"per annum\",\n",
    "                        \"p.m\" : \"after midday\",\n",
    "                        \"pm\" : \"prime minister\",\n",
    "                        \"poc\" : \"people of color\",\n",
    "                        \"pov\" : \"point of view\",\n",
    "                        \"pp\" : \"pages\",\n",
    "                        \"ppl\" : \"people\",\n",
    "                        \"prw\" : \"parents are watching\",\n",
    "                        \"ps\" : \"postscript\",\n",
    "                        \"pt\" : \"point\",\n",
    "                        \"ptb\" : \"please text back\",\n",
    "                        \"pto\" : \"please turn over\",\n",
    "                        \"qpsa\" : \"what happens\", #\"que pasa\",\n",
    "                        \"ratchet\" : \"rude\",\n",
    "                        \"rbtl\" : \"read between the lines\",\n",
    "                        \"rlrt\" : \"real life retweet\", \n",
    "                        \"rofl\" : \"rolling on the floor laughing\",\n",
    "                        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "                        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "                        \"rt\" : \"retweet\",\n",
    "                        \"ruok\" : \"are you ok\",\n",
    "                        \"sfw\" : \"safe for work\",\n",
    "                        \"sk8\" : \"skate\",\n",
    "                        \"smh\" : \"shake my head\",\n",
    "                        \"sq\" : \"square\",\n",
    "                        \"srsly\" : \"seriously\", \n",
    "                        \"ssdd\" : \"same stuff different day\",\n",
    "                        \"tbh\" : \"to be honest\",\n",
    "                        \"tbs\" : \"tablespooful\",\n",
    "                        \"tbsp\" : \"tablespooful\",\n",
    "                        \"tfw\" : \"that feeling when\",\n",
    "                        \"thks\" : \"thank you\",\n",
    "                        \"tho\" : \"though\",\n",
    "                        \"thx\" : \"thank you\",\n",
    "                        \"tia\" : \"thanks in advance\",\n",
    "                        \"til\" : \"today i learned\",\n",
    "                        \"tl;dr\" : \"too long i did not read\",\n",
    "                        \"tldr\" : \"too long i did not read\",\n",
    "                        \"tmb\" : \"tweet me back\",\n",
    "                        \"tntl\" : \"trying not to laugh\",\n",
    "                        \"ttyl\" : \"talk to you later\",\n",
    "                        \"u\" : \"you\",\n",
    "                        \"u2\" : \"you too\",\n",
    "                        \"u4e\" : \"yours for ever\",\n",
    "                        \"utc\" : \"coordinated universal time\",\n",
    "                        \"w/\" : \"with\",\n",
    "                        \"w/o\" : \"without\",\n",
    "                        \"w8\" : \"wait\",\n",
    "                        \"wassup\" : \"what is up\",\n",
    "                        \"wb\" : \"welcome back\",\n",
    "                        \"wtf\" : \"what the fuck\",\n",
    "                        \"wtg\" : \"way to go\",\n",
    "                        \"wtpa\" : \"where the party at\",\n",
    "                        \"wuf\" : \"where are you from\",\n",
    "                        \"wuzup\" : \"what is up\",\n",
    "                        \"wywh\" : \"wish you were here\",\n",
    "                        \"yd\" : \"yard\",\n",
    "                        \"ygtr\" : \"you got that right\",\n",
    "                        \"ynk\" : \"you never know\",\n",
    "                        \"zzz\" : \"sleeping bored and tired\"\n",
    "                        }\n",
    "            \n",
    "        sample_typos_slang_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_typos_slang.keys()) + r')(?!\\w)')\n",
    "        sample_acronyms_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_acronyms.keys()) + r')(?!\\w)')\n",
    "        sample_abbr_pattern = re.compile(r'(?<!\\w)(' + '|'.join(re.escape(key) for key in sample_abbr.keys()) + r')(?!\\w)')\n",
    "        \n",
    "        text = sample_typos_slang_pattern.sub(lambda x: sample_typos_slang[x.group()], text)\n",
    "        text = sample_acronyms_pattern.sub(lambda x: sample_acronyms[x.group()], text)\n",
    "        text = sample_abbr_pattern.sub(lambda x: sample_abbr[x.group()], text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "train_data['text_clean'] = train['comment_text'].apply(lambda x: cleaning(x))\n",
    "val_df['text_clean'] = val_df['comment_text'].apply(lambda x: cleaning(x))\n",
    "test_data['text_clean'] = test['comment_text'].apply(lambda x: cleaning(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìë Part of Speech Tagging & Lemmatizzazione**\n",
    "Successivamente alla pulizia del testo si pu√≤ procedere al **Part of Speech Tagging**, cio√® quell'operazione che associa ad ogni parola un tag tra i seguenti:\n",
    "- N: noun (nome)\n",
    "- V: verb (verbo)\n",
    "- J: adj (aggettivo)\n",
    "- R: adv (avverbio)\n",
    "\n",
    "Questo permette all'operazione successiva, la **lemmatizzazione**, di avvenire in maniera migliore.\n",
    "\n",
    "La lemmatizzazione √® quell'operazione che porta tutti i sostantivi alla forma base, per esempio i verbi vengono tutti portati all'infinito e gli aggettivi vengono portati tutti alla forma base, andando a modificare eventuali superlativi etc.  \n",
    "Abbiamo scelto di effettuare la lemmatizzazione anzich√© solo uno stemming in quanto abbiamo valutato che, per i nostri scopi, informazioni come il tempo verbale non fossero rilevanti, al contratio l'utilizzo di pi√π parole per veicolare lo stesso messaggio avrebbe solo aggiunto rumore al nostro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import brown\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('brown')\n",
    "\n",
    "# Part of Speech Tagging\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \n",
    "               \"V\":wordnet.VERB, \n",
    "               \"J\":wordnet.ADJ, \n",
    "               \"R\":wordnet.ADV\n",
    "              }\n",
    "    \n",
    "train_sents = brown.tagged_sents(categories='news')\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "\n",
    "def pos_tag_wordnet(text, pos_tag_type=\"pos_tag\"):\n",
    "    \"\"\"\n",
    "        Create pos_tag with wordnet format\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_tagged_text = t2.tag(text)\n",
    "    \n",
    "    # map the pos tagging output with wordnet output \n",
    "    pos_tagged_text = [(word, wordnet_map.get(pos_tag[0])) if pos_tag[0] in wordnet_map.keys() else (word, wordnet.NOUN) for (word, pos_tag) in pos_tagged_text ]\n",
    "   \n",
    "    return pos_tagged_text\n",
    "\n",
    "# Lemmatization\n",
    "def lemmatize_word(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = [lemmatizer.lemmatize(word, tag) for word, tag in text]\n",
    "    return lemma\n",
    "\n",
    "# Apply Pos Tagging\n",
    "train_data['separated'] = train_data['text_clean'].apply(lambda x: [x for x in x.split()])\n",
    "train_data['text_pos'] = train_data['separated'].apply(lambda x: pos_tag_wordnet(x)) \n",
    "val_df['separated'] = val_df['text_clean'].apply(lambda x: [x for x in x.split()])\n",
    "val_df['text_pos'] = val_df['separated'].apply(lambda x: pos_tag_wordnet(x))\n",
    "test_data['separated'] = test_data['text_clean'].apply(lambda x: [x for x in x.split()])\n",
    "test_data['text_pos'] = test_data['separated'].apply(lambda x: pos_tag_wordnet(x))\n",
    "\n",
    "# Apply Lemmatization\n",
    "train_data['text_lem_wpos'] = train_data['text_pos'].apply(lambda x: lemmatize_word(x))\n",
    "train_data['text_lem'] = [' '.join(map(str,l)) for l in train_data['text_lem_wpos']]\n",
    "val_df['text_lem_wpos'] = val_df['text_pos'].apply(lambda x: lemmatize_word(x))\n",
    "val_df['text_lem'] = [' '.join(map(str,l)) for l in val_df['text_lem_wpos']]\n",
    "test_data['text_lem_wpos'] = test_data['text_pos'].apply(lambda x: lemmatize_word(x))\n",
    "test_data['text_lem'] = [' '.join(map(str,l)) for l in test_data['text_lem_wpos']]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis (Pt.2)\n",
    "Dopo un primo preprocessing possiamo effettuare una seconda analisi dei dati, andando a creare delle wordcloud per l'intero dataset e per ogni singola classe di tossicit√†."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/train_clean.csv', index=False)\n",
    "test.to_csv('data/test_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "mask = np.array(Image.open('./images/wikipedia_mask.jpg'))\n",
    "\n",
    "def generate_wordcloud(df, clm):\n",
    "    text = []\n",
    "    comments = train_data.loc[df[clm] == 1]['text_clean']\n",
    "\n",
    "    for c in comments:\n",
    "        text.append(c) \n",
    "    words = ' '.join(text)\n",
    "    return WordCloud(stopwords=stop, background_color='white', mask=mask, height=1500, width=1500).generate(words)\n",
    "\n",
    "train_toxic = generate_wordcloud(train_data, 'toxic')\n",
    "train_sev_toxic = generate_wordcloud(train_data, 'severe_toxic')\n",
    "train_obscene = generate_wordcloud(train_data, 'obscene')\n",
    "train_threat = generate_wordcloud(train_data, 'threat')\n",
    "train_insult = generate_wordcloud(train_data, 'insult')\n",
    "train_id_hate = generate_wordcloud(train_data, 'identity_hate')\n",
    "train_general = WordCloud(stopwords=stop, background_color='white', height=1500, width=4500).generate(\" \".join(train['text_clean']))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(45, 10), gridspec_kw = {'wspace':0.01, 'hspace':0.1})\n",
    "axes.imshow(train_general)\n",
    "axes.axis('off')\n",
    "axes.set_title('General Word Cloud')\n",
    "\n",
    "plt.show()\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10), gridspec_kw = {'wspace':0.01, 'hspace':0.1})\n",
    "\n",
    "\n",
    "axes[0][0].imshow(train_toxic)\n",
    "axes[0][0].axis('off')\n",
    "axes[0][0].set_title('Toxic Word Cloud')\n",
    "axes[0][0].set_aspect('equal')\n",
    "\n",
    "axes[0][1].imshow(train_sev_toxic)\n",
    "axes[0][1].axis('off')\n",
    "axes[0][1].set_title('Severely Toxic Word Cloud')\n",
    "axes[0][1].set_aspect('equal')\n",
    "\n",
    "axes[0][2].imshow(train_obscene)\n",
    "axes[0][2].axis('off')\n",
    "axes[0][2].set_title('Obscene Word Cloud')\n",
    "axes[0][2].set_aspect('equal')\n",
    "\n",
    "axes[1][0].imshow(train_threat)\n",
    "axes[1][0].axis('off')\n",
    "axes[1][0].set_title('Threat Word Cloud')\n",
    "axes[1][0].set_aspect('equal')\n",
    "\n",
    "axes[1][1].imshow(train_insult)\n",
    "axes[1][1].axis('off')\n",
    "axes[1][1].set_title('Insult Word Cloud')\n",
    "axes[1][1].set_aspect('equal')\n",
    "\n",
    "axes[1][2].imshow(train_id_hate)\n",
    "axes[1][2].axis('off')\n",
    "axes[1][2].set_title('Identity Hate Word Cloud')\n",
    "axes[1][2].set_aspect('equal')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvataggio del dataset pulito\n",
    "Commentato per non creare problemi nel momento in cui si vuole eseguire il notebook da zero eseguendo tutte le celle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['text_lem', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "#train_data.to_csv('data/train_clean.csv', columns = header, index = False)\n",
    "#test_wlabels.to_csv('data/test_clean.csv', columns = header, index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dati gi√† puliti\n",
    "Commentato per non creare problemi nel momento in cui si vuole eseguire il notebook da zero eseguendo tutte le celle.  \n",
    "Nel caso si voglia caricare il dataset gi√† pulito, basta decommentare la cella e far partire l'esecuzione dalla cella sottostante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = pd.read_csv('data/train_clean.csv')\n",
    "#test_data = pd.read_csv('data/test_clean.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETE NEURALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_df['text_lem']))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_df['text_lem'])\n",
    "list_tokenized_val = tokenizer.texts_to_sequences(val_df['text_lem'])\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test_wlabels['text_lem'])\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_val = pad_sequences(list_tokenized_val, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "# save the tokenizer json in a file\n",
    "import json\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('data/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\dla\\neural network\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3473: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "if not os.path.exists('data/glove.6B.50d.txt'):\n",
    "    with zipfile.ZipFile('data/glove.6B.50d.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data/')\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(\"data/glove.6B.50d.txt\", encoding=\"utf8\"))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# save embedding matrix\n",
    "np.save('data/embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2165/2165 [==============================] - ETA: 0s - loss: 0.2011 - accuracy: 0.9102\n",
      "Epoch 1: saving model to training\\cp.ckpt\n",
      "2165/2165 [==============================] - 131s 59ms/step - loss: 0.2011 - accuracy: 0.9102 - val_loss: 0.0658 - val_accuracy: 0.9947\n",
      "Epoch 2/5\n",
      "2165/2165 [==============================] - ETA: 0s - loss: 0.1558 - accuracy: 0.9421\n",
      "Epoch 2: saving model to training\\cp.ckpt\n",
      "2165/2165 [==============================] - 126s 58ms/step - loss: 0.1558 - accuracy: 0.9421 - val_loss: 0.0591 - val_accuracy: 0.9900\n",
      "Epoch 3/5\n",
      "2165/2165 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9215\n",
      "Epoch 3: saving model to training\\cp.ckpt\n",
      "2165/2165 [==============================] - 125s 58ms/step - loss: 0.1415 - accuracy: 0.9215 - val_loss: 0.0582 - val_accuracy: 0.9948\n",
      "Epoch 4/5\n",
      "2165/2165 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.8982\n",
      "Epoch 4: saving model to training\\cp.ckpt\n",
      "2165/2165 [==============================] - 123s 57ms/step - loss: 0.1300 - accuracy: 0.8982 - val_loss: 0.0541 - val_accuracy: 0.9510\n",
      "Epoch 5/5\n",
      "2165/2165 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.8302\n",
      "Epoch 5: saving model to training\\cp.ckpt\n",
      "2165/2165 [==============================] - 124s 57ms/step - loss: 0.1193 - accuracy: 0.8302 - val_loss: 0.0619 - val_accuracy: 0.7971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x230cd7114c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "\n",
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,verbose=1)\n",
    "\n",
    "model.fit(X_t, y, validation_data=(X_val, val_df[list_classes].values), batch_size=16, epochs=5, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x230cd390308>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a model with custom weights\n",
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "load_model = Model(inputs=inp, outputs=x)\n",
    "load_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = load_model.predict([X_te], batch_size=16, verbose=1)\n",
    "\n",
    "# Convert to binary\n",
    "outputs =[]\n",
    "for p in range(len(y_test)):\n",
    "            aux = []\n",
    "            for l in range(6):\n",
    "                aux.append(int(y_test[p][l] >= 0.5))\n",
    "            outputs.append(aux)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    http://stackoverflow.com/q/32239577/395857\n",
    "    \n",
    "    Take in np.array for y_true and y_pred. E.g.\n",
    "    y_true = np.array([[0,1,0],\n",
    "                       [0,1,1],\n",
    "                       [1,0,1],\n",
    "                       [0,0,1]])\n",
    "\n",
    "    y_pred = np.array([[0,1,1],\n",
    "                       [0,1,1],\n",
    "                       [0,1,0],\n",
    "                       [0,0,0]])\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8154193629059989\n",
      "Accuracy: 0.9517125678618692\n"
     ]
    }
   ],
   "source": [
    "# Metrics\n",
    "print(hamming_score( test_wlabels[list_classes].values, outputs))\n",
    "accuracy = []\n",
    "test_label = test_wlabels[list_classes].to_numpy()\n",
    "\n",
    "for p in range(len(outputs)):\n",
    "    correct_val = 0\n",
    "    for i in range(6):\n",
    "        if outputs[p][i] == test_label[p,i]:\n",
    "            correct_val += 1\n",
    "    accuracy.append(correct_val/6)\n",
    "\n",
    "print(f'Accuracy: {np.mean(accuracy)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "import json\n",
    "\n",
    "def model_pretrained():\n",
    "    load_model = None\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    layers = Dense(6, activation=\"sigmoid\")(x)\n",
    "    checkpoint_path = \"training/cp.ckpt\"\n",
    "    load_model = Model(inputs=inp, outputs=layers)\n",
    "    load_model.load_weights(checkpoint_path)\n",
    "    return load_model\n",
    "\n",
    "def load_tokenizer():\n",
    "    tokenizer_load = None\n",
    "    with open('data/tokenizer.json') as f:\n",
    "        data = json.load(f)\n",
    "        tokenizer_load  = tokenizer_from_json(data)\n",
    "    return tokenizer_load\n",
    "\n",
    "def try_me(string):\n",
    "    string = cleaning(string)\n",
    "    model_loaded = model_pretrained()\n",
    "    tokenizer_load = load_tokenizer()\n",
    "    list_tokenized_string = tokenizer_load.texts_to_sequences([string])\n",
    "    X_string = pad_sequences(list_tokenized_string, maxlen=maxlen)\n",
    "    y_string = model_loaded.predict([X_string], batch_size=16, verbose=1)\n",
    "    aux = []\n",
    "    for l in range(6):\n",
    "        aux.append(int(y_string[0][l] >= 0.5))\n",
    "    \n",
    "    print_string = f'Toxic: {bool(aux[0])} \\nSevere Toxic: {bool(aux[1])} \\nObscene: {bool(aux[2])} \\nThreat: {bool(aux[3])} \\nInsult: {bool(aux[4])} \\nIdentity Hate: {bool(aux[5])}'\n",
    "\n",
    "    return print(print_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_me('You are a stupid idiot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (tags/v3.8.8:024d805, Feb 19 2021, 13:18:16) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5a436c83699a430054d97f0c44aa5446642b8be47a6e5330eb70a885247fe95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
